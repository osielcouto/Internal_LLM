# Internal_LLM
ğŸ“š AvaliaÃ§Ã£o de Bases de Dados TÃ©cnicas com Modelos de Linguagem de Grande Escala:  Um Estudo Aplicado ao Wi-Fi 7 ğŸ“š
ğŸ‘¨ğŸ½â€ğŸ’» Autor: Osiel do Couto Rosa  ğŸ‘¨ğŸ½â€ğŸ’»

Um sistema de perguntas e respostas baseado em seus documentos PDF, comparando diferentes modelos de IA.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ PRÃ‰-REQUISITOS (ANTES DE COMEÃ‡AR)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Hardware
	â€¢ MemÃ³ria RAM de pelo menos 16GB

2. TERMINAL ABERTO:
   â€¢ Windows: Pressione Win+R, digite "cmd" e Enter
   â€¢ Mac/Linux: Abra "Terminal"

3. PYTHON INSTALADO (3.8 ou superior):
   â€¢ Verifique se jÃ¡ tem: digite no terminal:
     python --version
   â€¢ Se nÃ£o tiver, baixe em: python.org/downloads

4. OLLAMA INSTALADO:
   â€¢ Baixe em: ollama.ai/download
   
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸš€ COMO INSTALAR AS BIBLIOTECAS NECESSÃRIAS (PASSO A PASSO)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. INSTALE AS BIBLIOTECAS (digite cada linha e aguarde):
   pip install langchain-community langchain-ollama
   pip install pypdf
   pip install huggingface-hub
   pip install pandas
   pip install tabulate
   pip install faiss-cpu

2. BAIXE OS MODELOS DE IA (digite cada um separadamente):
   ollama pull llama2
   ollama pull mistral
   ollama pull tinyllama

3. COLOQUE SEUS DOCUMENTOS PDF na pasta:
   /base_dados/ (crie esta pasta na mesma localizaÃ§Ã£o do arquivo .py) -> Ajuste no cÃ³digo principal o caminho da pasta
   
4. ARQUIVO DE PERGUNTAS
	Para o modo "Bateria de testes" Ã© necessÃ¡rio um arquivo .csv com as seguintes caracteristicas
	Linha 1: "Perguntas"
	Linha 2-n: Perguntas que deseja enviar para os LLM
	Ãšltima linha: Flag "/end"
	Modelo:
		Perguntas
		Texto pergunta 1
		Texto pergunta 2
		...
		/end

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’» COMO EXECUTAR O PROGRAMA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. INICIE O SERVIDOR DO OLLAMA:
   1 - Abra o terminal
   2 - Digite o comando abaixo 
		ollama serve
   
2. INICIE O SISTEMA:
   1 - Abra outro terminal em paralelo
   2 - Navegue atÃ© sua pasta de trabalho:
		cd caminho_da_sua_pasta
   3 - Execute o programa com o comando abaixo:
		python internal_llm.py
   
3. MENU PRINCIPAL aparecerÃ¡ com 4 opÃ§Ãµes:
   1 - Modo normal: FaÃ§a perguntas a um modelo especÃ­fico
   2 - Benchmark: Testa TODOS modelos automaticamente
   3 - Bateria de testes: Executa uma bateria de testes automatizada com perguntas prÃ©-definidas em um arquivo csv.
   4 - Sair

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“‚ ESTRUTURA DE ARQUIVOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/sua_pasta/
   â”‚
   â”œâ”€â”€ base_dados/       (Coloque seus PDFs aqui)
   â”œâ”€â”€ internal_llm.py   (Arquivo principal)
   â”œâ”€â”€ perguntas.csv  	 (Arquivo de perguntas para ser usado na bateria de testes)
   â””â”€â”€ faiss_index/      (Pasta criada automaticamente)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â“ PROBLEMAS COMUNS (SOLUÃ‡Ã•ES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ”´ Erro "ModuleNotFoundError":
   â€¢ Significa que falta alguma biblioteca
   â€¢ Reinstale todas conforme orientaÃ§Ã£o

ğŸ”´ Ollama nÃ£o responde:
   â€¢ Verifique se o serviÃ§o estÃ¡ rodando em OUTRO terminal:
     ollama serve
   â€¢ Teste manualmente:
	ollama run llama2

ğŸ”´ PDFs nÃ£o sÃ£o reconhecidos:
   â€¢ Verifique se estÃ£o na pasta /base_dados/
   â€¢ Nomes de arquivos nÃ£o devem ter acentos ou espaÃ§os
