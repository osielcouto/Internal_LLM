"""
Avalia√ß√£o de Bases de Dados T√©cnicas com Modelos de Linguagem de Grande Escala:  Um Estudo Aplicado ao Wi-Fi 7
Autor: Osiel do Couto Rosa

Este sistema permite:
1. Testar individualmente modelos LLM com acesso a documentos (RAG)
2. Comparar o desempenho de m√∫ltiplos modelos (com e sem RAG)
3. Executar baterias de testes automatizadas com lista de perguntas pr√©-definidas
4. Gerar relat√≥rios com informa√ß√µes de conte√∫do, fontes e tempo de resposta

Configura√ß√µes principais podem ser ajustadas nas constantes no in√≠cio do c√≥digo.
"""

import time
from datetime import datetime
from tabulate import tabulate
import pandas as pd
import os
from typing import Tuple
import warnings
from langchain_core._api.deprecation import LangChainDeprecationWarning
from langchain_community.llms import Ollama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import CharacterTextSplitter

# ==============================================
# CONFIGURA√á√ïES PRINCIPAIS (AJUST√ÅVEIS)
# ==============================================

# Diret√≥rio contendo os documentos em PDF
PDF_DIR = "./base_tcc"  # Caminho da pasta de documentos

# Nome do √≠ndice FAISS para armazenar os embeddings
FAISS_INDEX = "faiss_index"  # Pode ser alterado se necess√°rio

# Modelos dispon√≠veis e suas descri√ß√µes
# Nota: "llama2_raw" √© um termo especial para usar Llama 2 sem documentos
AVAILABLE_MODELS = {
    "mistral": "Mistral (4.4GB) - Melhor qualidade para tarefas complexas",
    "llama2": "Llama 2 (3.8GB) - Equil√≠brio entre qualidade e desempenho",
    "tinyllama": "TinyLlama (637MB) - Leve e r√°pido, para testes r√°pidos",
    "llama2_raw": "Llama 2 Controle (3.8GB) - Sem documentos (usa Llama 2 puro)"
}

# Configura√ß√µes de chunking para o processamento de documentos
CHUNK_SIZE = 500       # Tamanho dos peda√ßos de texto (em caracteres)
CHUNK_OVERLAP = 100    # Sobreposi√ß√£o entre chunks (para manter contexto)

# Modelo de embeddings
EMBEDDINGS_MODEL = "all-MiniLM-L6-v2"

# ==============================================
# FUN√á√ïES PRINCIPAIS
# ==============================================

def select_model():
    """
    Permite ao usu√°rio selecionar interativamente um modelo LLM.
    
    Retorna:
        str: Nome do modelo selecionado (chave em AVAILABLE_MODELS)
    
    Comportamento:
        - Exibe lista numerada de modelos dispon√≠veis
        - Valida a entrada do usu√°rio
        - Verifica se o modelo est√° instalado (exceto para llama2_raw)
        - Permite tentar novamente em caso de erro
    """
    print("\nüîç Modelos LLM dispon√≠veis:")
    for i, (name, desc) in enumerate(AVAILABLE_MODELS.items(), 1):
        print(f"{i}. {name.ljust(10)} - {desc}")
    
    while True:
        try:
            choice = int(input(f"\nSelecione o modelo (1-{len(AVAILABLE_MODELS)}): "))
            if 1 <= choice <= len(AVAILABLE_MODELS):
                selected = list(AVAILABLE_MODELS.keys())[choice-1]
                
                # Verifica√ß√£o especial para o modelo raw (n√£o ser√° checada a instala√ß√£o devido ao termo especial)
                if selected == "llama2_raw":
                    return selected
                
                # Para outros modelos, verifica se est√° instalado
                try:
                    Ollama(model=selected).invoke("Teste")                                              # M√©todo invoke envia uma solicita√ß√£o para o modelo para verificar se est√° instalado
                    return selected
                except Exception:
                    print(f"‚ùå Modelo '{selected}' n√£o encontrado. Baixe com: ollama pull {selected}")  # Se o invoke apresentar erro, solicita instala√ß√£o via ollama
                    continue    
                    
            print("‚ö†Ô∏è Op√ß√£o inv√°lida!")
        except ValueError:
            print("‚ö†Ô∏è Digite apenas n√∫meros!")

def load_documents():
    """
    Carrega e processa todos os documentos PDF do diret√≥rio configurado.
    
    Retorna:
        list: Lista de documentos carregados (langchain Document objects)
    
    Comportamento:
        - Varre o diret√≥rio PDF_DIR em busca de arquivos .pdf
        - Usa PyPDFLoader para extrair texto de cada p√°gina
        - Retorna lista consolidada de todos os documentos
        - Exibe erros individuais sem interromper o processo
    """
    print("\nüìÇ Carregando e processando documentos...")
    documents = []                                                      # Inicializa a lista de documentos
    for pdf_file in os.listdir(PDF_DIR):                                # Loop nos arquivos do diret√≥rio
        if pdf_file.endswith(".pdf"):                                   # Valida se s√£o arquivos .pdf
            try:
                print(f"  - Processando: {pdf_file}")
                loader = PyPDFLoader(os.path.join(PDF_DIR, pdf_file))   # Extrai o conte√∫do do PDF e retorna uma lista de objetos Document
                documents.extend(loader.load())                         # Adiciona os documentos √† lista principal
            except Exception as e:
                print(f"‚ùå Erro no arquivo {pdf_file}: {str(e)}")
    return documents

def setup_rag_system(model_name):
    """
    Configura o sistema RAG completo para um modelo espec√≠fico.
    
    Par√¢metros:
        model_name (str): Nome do modelo a ser usado
    
    Retorna:
        tupla: (vectorstore, llm) - Componentes do sistema RAG
    
    Comportamento:
        - Para 'llama2_raw', retorna (None, None) pois n√£o usa RAG
        - Processa documentos em chunks
        - Cria/recupera armazenamento vetorial FAISS
        - Configura o modelo LLM com par√¢metros espec√≠ficos
    """
    # Caso especial para o modelo raw (n√£o configura RAG)
    if model_name == "llama2_raw":
        return None, None
    
    # 1. Processamento de documentos e dividindo em peda√ßos menores
    print("\nüîß Configurando sistema RAG...")
    text_splitter = CharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separator="\n"
    )
    chunks = text_splitter.split_documents(load_documents())
    
    # 2. Configura√ß√£o do armazenamento vetorial
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)
    
    # Tenta carregar √≠ndice existente ou cria novo
    if os.path.exists(FAISS_INDEX):
        print("  - Carregando √≠ndice FAISS existente...")
        vectorstore = FAISS.load_local(FAISS_INDEX, embeddings, allow_dangerous_deserialization=True)
    else:
        print("  - Criando novo √≠ndice FAISS...")
        vectorstore = FAISS.from_documents(chunks, embeddings)
        vectorstore.save_local(FAISS_INDEX)
    
    # 3. Configura√ß√£o do modelo LLM
    print(f"  - Inicializando modelo {model_name}...")
    llm = Ollama(
        model=model_name,
        temperature=0.3,  # Controla a criatividade (0 = mais determin√≠stico)
        system="""
        Voc√™ √© um assistente acad√™mico especializado em redes sem fio. Suas respostas devem ser em portugu√™s do Brasil e baseadas 
        estritamente nos documentos fornecidos, com an√°lise cr√≠tica e reda√ß√£o original. Responda com foco 
        em Wi-Fi 7 (IEEE 802.11be), explicando os conceitos de maneira clara, objetiva e tecnicamente 
        precisa, sem recorrer a conhecimento externo. Utilize apenas as informa√ß√µes presentes nos documentos, 
        mantendo o rigor t√©cnico, evitando jarg√µes excessivos e garantindo que a resposta seja interpret√°vel 
        por profissionais da √°rea t√©cnica ou estudantes avan√ßados.
        """,
        num_thread=4 if model_name != "tinyllama" else 2  # Ajuste de desempenho, pesquisas mostraram que usar o tinyllama com menos threads evita overhead e otimiza os resultados
    )
    
    return vectorstore, llm

def generate_response(question: str, vectorstore: FAISS, llm: Ollama) -> Tuple[str, list, float]:
    """
    Gera uma resposta usando o sistema RAG.
    
    Par√¢metros:
        question (str): Pergunta do usu√°rio
        vectorstore (FAISS): Armazenamento vetorial de documentos
        llm (Ollama): Modelo LLM configurado
    
    Retorna:
        tupla: (resposta, documentos_relevantes, tempo_execucao)
    
    Comportamento:
        - Busca trechos relevantes nos documentos
        - Constr√≥i prompt contextualizado
        - Invoca o modelo LLM
        - Mede tempo de execu√ß√£o
    """
    start_time = time.time()                            # Inicia contagem de tempo
    
    # 1. Busca os trechos mais relevantes
    docs = vectorstore.similarity_search(question, k=5)  # Top 5 resultados(pode ser alterado)
    
    # 2. Constr√≥i o contexto para o prompt
    context = "\n\n".join([
        f"Fonte: {os.path.basename(doc.metadata['source'])} (P√°gina {doc.metadata.get('page', 'N/A')})\n" # Formata cada documento exibindo Fonte e P√°gina
        f"Conte√∫do: {doc.page_content}\n------"                                                           # Formata cada documento exibidno conte√∫do e separa com ------
        for doc in docs
    ])
    
    # 3. Monta o prompt final
    prompt = f"""Com base nestes trechos:
    {context}
    Responda √† pergunta abaixo com suas pr√≥prias palavras.
    Pergunta: {question}"""
    
    # 4. Invoca o modelo LLM enviando o contexto
    response = llm.invoke(prompt)
    elapsed_time = round(time.time() - start_time, 2)    # Finaliza contagem de tempo
    
    return response, docs, elapsed_time            

def generate_raw_response(question: str, model_name: str = "llama2") -> Tuple[str, float]:
    """
    Gera resposta SEM usar documentos (vers√£o de controle do modelo).
    
    Par√¢metros:
        question (str): Pergunta do usu√°rio
        model_name (str): Nome do modelo a ser usado (padr√£o: "llama2")
    
    Retorna:
        tupla: (resposta, tempo_execucao)
    
    Comportamento:
        - Configura o modelo
        - Invoca o modelo sem contexto de documentos
        - Mede tempo de execu√ß√£o
    """
    start_time = time.time()                    # Inicia contagem de tempo
    
    llm = Ollama(
        model=model_name,
        temperature=0.3,
        system="""
        Voc√™ √© um assistente t√©cnico especializado em redes sem fio, com foco em Wi-Fi 7 (IEEE 802.11be). 
        Responda sempre em portugu√™s do Brasil de forma clara, objetiva e acess√≠vel, utilizando termos corretos, mas explicando conceitos 
        de maneira simples. Evite jarg√µes excessivos. Seja direto, did√°tico e preciso, como se estivesse 
        explicando para um profissional da √°rea t√©cnica que deseja respostas r√°pidas, por√©m compreens√≠veis 
        por qualquer pessoa com conhecimento b√°sico em tecnologia. Responda com conhecimento geral 
        (n√£o use documentos espec√≠ficos).
        """
    )
    
    response = llm.invoke(question)                        # Envia pergunta diretamente ao modelo sem contexto
    elapsed_time = round(time.time() - start_time, 2)      # Finaliza contagem de tempo
    
    return response, elapsed_time

# ==============================================
# MODOS DE OPERA√á√ÉO
# ==============================================

def benchmark_interativo():
    """
    Modo de benchmark interativo que testa todos os modelos com a mesma pergunta.
    
    Comportamento:
        - Permite ao usu√°rio fazer perguntas iterativamente
        - Para cada pergunta, testa todos os modelos (com e sem RAG)
        - Exibe resultados comparativos em tempo real
        - Salva relat√≥rio CSV ao final
    """
    print("\n‚ö° MODO BENCHMARK AUTOM√ÅTICO")
    print("Testar√° sequencialmente:\n"
          "1. Llama2 (CONTROLE)\n"
          "2. Mistral (RAG)\n"
          "3. Llama2 (RAG)\n"
          "4. TinyLlama (RAG)\n")
    
    # Pr√©-carrega todos os componentes para melhor desempenho
    print("\nüìÇ Carregando documentos...")
    documents = load_documents()
    
    print("üîß Preparando sistema de embeddings...")
    text_splitter = CharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separator="\n"
    )
    chunks = text_splitter.split_documents(documents)
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)
    
    print("üîß Configurando vectorstore...")
    if os.path.exists(FAISS_INDEX):
        vectorstore = FAISS.load_local(FAISS_INDEX, embeddings, allow_dangerous_deserialization=True)
    else:
        vectorstore = FAISS.from_documents(chunks, embeddings)
        vectorstore.save_local(FAISS_INDEX)
    
    # DataFrame para armazenar resultados para exporta√ß√£o posterior
    df_resultados = pd.DataFrame(columns=[
        "Modelo", 
        "Pergunta", 
        "Tempo(s)", 
        "Resposta",
        "Fontes"
    ])
    
    # Loop principal de perguntas
    while True:
        question = input("\nüîç Digite a pergunta para comparar (ou 'sair'): ").strip()
        if question.lower() == 'sair':
            break
            
        print("\n‚è≥ Processando nos 4 modelos...")
        
        # 1. Testa vers√£o de controle (sem documentos)
        try:
            print("  - Testando llama2 (CONTROLE)...", end='\r')
            response, tempo = generate_raw_response(question)
            df_resultados.loc[len(df_resultados)] = [
                "llama2 (CONTROLE)",
                question[:80] + ("..." if len(question) > 80 else ""),
                tempo,
                response.strip()[:500] + ("..." if len(response) > 500 else ""),
                "N/A (resposta controle)"
            ]
            print(f"‚úÖ llama2 (CONTROLE)   | {str(tempo).ljust(5)}s")
        except Exception as e:
            print(f"‚ùå Falha em llama2 (CONTROLE): {str(e)}")
        
        # 2. Testa modelos com RAG
        modelos_rag = [
            ("mistral", "Mistral (RAG)"),
            ("llama2", "Llama2 (RAG)"),
            ("tinyllama", "TinyLlama (RAG)")
        ]
        
        for model_name, nome_exibicao in modelos_rag:
            try:
                print(f"  - Testando {nome_exibicao.ljust(12)}...", end='\r')
                
                # Configura LLM espec√≠fico
                llm = Ollama(
                    model=model_name,
                    temperature=0.3,
                    system="""
        Voc√™ √© um assistente acad√™mico especializado em redes sem fio. Suas respostas devem ser em portugu√™s do Brasil e baseadas 
        estritamente nos documentos fornecidos, com an√°lise cr√≠tica e reda√ß√£o original. Responda com foco 
        em Wi-Fi 7 (IEEE 802.11be), explicando os conceitos de maneira clara, objetiva e tecnicamente 
        precisa, sem recorrer a conhecimento externo. Utilize apenas as informa√ß√µes presentes nos documentos, 
        mantendo o rigor t√©cnico, evitando jarg√µes excessivos e garantindo que a resposta seja interpret√°vel 
        por profissionais da √°rea t√©cnica ou estudantes avan√ßados.
        """,
                    num_thread=4 if model_name != "tinyllama" else 2
                )
                
                # Gera resposta RAG
                response, docs, tempo = generate_response(question, vectorstore, llm)
                fontes = "; ".join([
                    f"{os.path.basename(d.metadata['source'])} (p.{d.metadata.get('page', 'N/A')})" 
                    for d in docs
                ])
                
                # Armazena resultados usando Pandas
                df_resultados.loc[len(df_resultados)] = [
                    nome_exibicao,
                    question[:80] + ("..." if len(question) > 80 else ""),          
                    tempo,
                    response.strip()[:500] + ("..." if len(response) > 500 else ""),
                    fontes
                ]
                
                print(f"‚úÖ {nome_exibicao.ljust(15)} | {str(tempo).ljust(5)}s")
                
            except Exception as e:
                print(f"‚ùå Falha em {nome_exibicao}: {str(e)}")
                continue
    
    # Salva e exibe resultados
    if not df_resultados.empty:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")                    # Obt√©m data e hora para nome do arquivo ser √∫nico
        nome_arquivo = f"benchmark_{timestamp}.csv"                             # Cria nome do arquivo_datahora
        df_resultados.to_csv(nome_arquivo, index=False, encoding='utf-8-sig')   # Exporta CSV 
        
        print("\nüìä RESULTADOS:")
        print(tabulate(df_resultados[["Modelo", "Tempo(s)"]],                   # Exibe resumo comparativo em tabela
                     headers="keys", 
                     tablefmt="simple"))
        
        print(f"\nüíæ Relat√≥rio salvo como: {nome_arquivo}")                      # Informa ao usu√°rio nome do arquivo salvo
    else:
        print("\n‚ö†Ô∏è Nenhum dado foi coletado")

def carregar_perguntas():
    """
    Carrega perguntas de um arquivo CSV com estrutura espec√≠fica.
    
    Formato esperado:
        - Primeira linha: cabe√ßalho "Perguntas"
        - Linhas seguintes: perguntas (uma por linha)
        - √öltima linha: "/end" (opcional)
    
    Retorna:
        list: Lista de perguntas carregadas
    """
    try:
        with open('perguntas.csv', 'r', encoding='utf-8') as file:          # Nome do arquivo a ser lido e codifica√ß√£o para caracteres especiais
            cabecalho = file.readline().strip()
            if cabecalho != "Perguntas":                                    # L√™ a primeira linha, remove espa√ßos e verifica cabe√ßalho
                print(f"‚ùå Erro: Cabe√ßalho inv√°lido. Esperado 'Perguntas', encontrado '{cabecalho}'")
                return []
            
            perguntas = [] 
            for linha in file:
                linha = linha.strip()                                       # Remove espa√ßo em branco
                if linha == "/end":                                         # Para ao encontrar a flag "End"
                    break
                
                if linha:
                    perguntas.append(linha)                                 # Adiciona linha a lista de perguntas
            
            if not perguntas:
                print("‚ùå Erro: Nenhuma pergunta encontrada no arquivo")
                return []
            
            if len(perguntas) != 100:                                       # Valida se todas as perguntas foram carregadas
                print(f"‚ö†Ô∏è Aviso: Esperadas 100 perguntas, encontradas {len(perguntas)}")
            
            if linha != "/end":                                             # Verifica se flag estava no arquivo
                print("‚ö†Ô∏è Aviso: Flag /end n√£o encontrada no final do arquivo")
            
            print(f"‚úÖ {len(perguntas)} perguntas carregadas com sucesso")   # Informa total de perguntas carregadas com sucesso ao usu√°rio
            return perguntas
            
    except FileNotFoundError:
        print("‚ùå Erro: Arquivo 'perguntas.csv' n√£o encontrado")
        print("  Certifique-se que o arquivo est√° na mesma pasta do script")
        return []
    except Exception as e:
        print(f"‚ùå Erro inesperado ao carregar perguntas: {str(e)}")
        return []

def executar_bateria_testes(model_name):
    """
    Executa uma bateria de testes automatizada com perguntas pr√©-definidas.
    
    Par√¢metros:
        model_name (str): Nome do modelo a ser testado
    
    Comportamento:
        - Carrega perguntas do arquivo
        - Para cada pergunta, gera resposta com o modelo selecionado
        - Armazena resultados detalhados
        - Gera relat√≥rio CSV ao final
    """
    perguntas = carregar_perguntas()
    if not perguntas:
        return
    
    # Configura√ß√£o especial para o modelo raw
    if model_name == "llama2_raw":
        print(f"\nüîß Iniciando teste com {model_name} (sem documentos) para {len(perguntas)} perguntas")
        
        resultados = []
        total_perguntas = len(perguntas)
        
        for i, pergunta in enumerate(perguntas, 1):               # Percorre lista de perguntas
            print(f"\nüìù Progresso: {i}/{total_perguntas}")       # Exibe progresso
            print(f"   Pergunta: {pergunta[:70]}...")             # Exibe trecho da pergunta limitando a 70 caracteres
            
            try:
                resposta, tempo = generate_raw_response(pergunta)
                
                resultados.append({                                 # Armazena resultados
                    'Modelo': model_name,
                    'Pergunta': pergunta,
                    'Tempo(s)': tempo,
                    'Resposta': resposta[:2000],
                    'Fontes': "N/A (resposta controle)"
                })
                
                print(f"   ‚úÖ Respondido em {resultados[-1]['Tempo(s)']}s")  # Exibe tempo de resposta
                
            except Exception as e:                                  # Captura erros caso ocorram
                print(f"   ‚ùå Erro: {str(e)}")
                resultados.append({
                    'Modelo': model_name,
                    'Pergunta': pergunta,
                    'Tempo(s)': 0,
                    'Resposta': f"ERRO: {str(e)}",
                    'Fontes': "N/A"
                })
    else:
        # Configura√ß√£o normal para modelos com RAG
        vectorstore, llm = setup_rag_system(model_name)
        print(f"\nüîß Iniciando teste com {model_name} para {len(perguntas)} perguntas")
        
        resultados = []
        total_perguntas = len(perguntas)
        
        for i, pergunta in enumerate(perguntas, 1):
            print(f"\nüìù Progresso: {i}/{total_perguntas}")
            print(f"   Pergunta: {pergunta[:70]}...")
            
            try:
                start_time = time.time()
                resposta, docs, tempo = generate_response(pergunta, vectorstore, llm)
                
                fontes = "; ".join(
                    f"{os.path.basename(d.metadata['source'])} (p.{d.metadata.get('page', 'N/A')})"
                    for d in docs
                )
                
                resultados.append({
                    'Modelo': model_name,
                    'Pergunta': pergunta,
                    'Tempo(s)': round(time.time() - start_time, 2),
                    'Resposta': resposta[:2000],
                    'Fontes': fontes
                })
                
                print(f"   ‚úÖ Respondido em {resultados[-1]['Tempo(s)']}s")
                
            except Exception as e:
                print(f"   ‚ùå Erro: {str(e)}")
                resultados.append({
                    'Modelo': model_name,
                    'Pergunta': pergunta,
                    'Tempo(s)': 0,
                    'Resposta': f"ERRO: {str(e)}",
                    'Fontes': "N/A"
                })
    
    # Salva resultados
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    nome_arquivo = f"resultados_{model_name}_{timestamp}.csv"
    
    pd.DataFrame(resultados).to_csv(nome_arquivo, index=False, encoding='utf-8-sig')
    print(f"\nüíæ Arquivo salvo: {nome_arquivo}")
    print(f"üìä Total de perguntas respondidas: {len([r for r in resultados if not r['Resposta'].startswith('ERRO')])}/{total_perguntas}")

# ==============================================
# Fun√ß√£o principal
# ==============================================

def main():
    """
    Fun√ß√£o principal que gerencia o menu interativo.
    
    Oferece tr√™s modos de opera√ß√£o:
    1. Modo individual: Testa um modelo por vez com perguntas interativas
    2. Benchmark comparativo: Compara todos os modelos com a mesma pergunta
    3. Bateria de testes: Executa perguntas pr√©-definidas em um modelo espec√≠fico
    """
    print("\n" + "="*50)
    print("SISTEMA DE COMPARA√á√ÉO DE MODELOS LLM".center(50))
    print("="*50)
    print("\nSelecione o modo de opera√ß√£o:")
    print("1. Modo normal (uso individual)")
    print("2. Benchmark comparativo (testar todos os modelos)")
    print("3. Bateria de testes")
    print("4. Sair")
    
    while True:
        escolha = input("\nDigite o modo desejado (1/2/3/4): ").strip()
        
        # Modo individual
        if escolha == "1":
            print("\nüõ†Ô∏è  Configura√ß√£o do Sistema")
            model_name = select_model()                 # Valida√ß√£o e sele√ß√£o do modelo 
            
            # Caso especial para o modelo raw
            if model_name == "llama2_raw":
                print(f"\n‚úÖ Sistema pronto! Modelo: {model_name} (sem documentos)")
                print("üîç Digite suas perguntas ou 'sair' para voltar ao menu.")
                
                while True:
                    question = input("\n‚ùì Pergunta: ").strip()
                    if question.lower() == 'sair':      # Converte todas as strings para minuscula e compara conte√∫do
                        break
                        
                    try:
                        print("\n‚è≥ Processando...")
                        response, time_sec = generate_raw_response(question)
                        
                        print(f"\nüí° Resposta ({time_sec}s):")
                        print(response)
                        
                    except Exception as e:
                        print(f"\n‚ùå Erro: {str(e)}")
                        print("Dica: Verifique se o servidor Ollama est√° rodando ('ollama serve')")
            else:
                # Configura√ß√£o normal para modelos com RAG
                vectorstore, llm = setup_rag_system(model_name)
                print(f"\n‚úÖ Sistema pronto! Modelo: {model_name}")
                print("üîç Digite suas perguntas ou 'sair' para voltar ao menu.")
                
                while True:
                    question = input("\n‚ùì Pergunta: ").strip()
                    if question.lower() == 'sair':
                        break
                        
                    try:
                        print("\n‚è≥ Processando...")
                        response, docs, time_sec = generate_response(question, vectorstore, llm)
                        
                        print(f"\nüí° Resposta ({time_sec}s):")
                        print(response)
                        
                        print("\nüìö Fontes utilizadas:")
                        for doc in docs:
                            print(f"- {os.path.basename(doc.metadata['source'])} (p√°gina {doc.metadata.get('page', 'N/A')})")
                            
                    except Exception as e:
                        print(f"\n‚ùå Erro: {str(e)}")
                        print("Dica: Verifique se o servidor Ollama est√° rodando ('ollama serve')")
            
        # Benchmark comparativo
        elif escolha == "2":
            benchmark_interativo()
            
        # Bateria de testes
        elif escolha == "3":
            model_name = select_model()
            try:
                executar_bateria_testes(model_name)
            except Exception as e:
                print(f"\n‚ùå Falha durante os testes: {str(e)}")
                if model_name != "llama2_raw":
                    print("Poss√≠veis causas:")
                    print("- Documentos n√£o encontrados no diret√≥rio especificado")
                    print("- √çndice FAISS corrompido")
                    print("- Problema de conex√£o com o Ollama")
                
        # Sair
        elif escolha == "4":
            print("\nüëã Encerrando o programa...")
            break
        else:
            print("‚ö†Ô∏è Op√ß√£o inv√°lida! Digite um n√∫mero entre 1 e 4.")

if __name__ == "__main__":
    # Suprime avisos de deprecia√ß√£o
    warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)
    
    # Verifica se o diret√≥rio de documentos existe
    if not os.path.exists(PDF_DIR):
        print(f"‚ùå Erro: Diret√≥rio de documentos '{PDF_DIR}' n√£o encontrado!")
        print("  Crie o diret√≥rio ou ajuste a vari√°vel PDF_DIR no c√≥digo.")
        exit(1)
    

    main()
